---
layout: post
title: Intro to GSP - A New Signal Processing Perspective For Graph Analysis
subtitle: Introduction to Graph Signal Processing
tags: [signal-processing, graph]
---

> At a very high level, DSP, and therefore GSP, study: 1) signals and their representations; 2) systems that process signals, usually referred to as filters; 3) signal transforms, including two very important ones, namely, the z-transform and the Fourier transform; and 4) sampling of signals, as well as other more specialized topics

Data can appear in various shapes. On one hand, natural scenes, such as in images, motion, video and sound, typically demonstrate continuityes, smoothness and regularity. T nature (images, video, sound, motion etc.) therefore impose prior knowledge that can be utilized with neat mathematical formulation for modeling. On the other hand, relational data usually comes with irregular properties, where different entities (nodes) may be connected with different and discrete relations that are encoded as numerical values.

Formally, a relational graph data is represented as  $$G=\{V,E,W_E\}$$ where $$V$$, $$E$$  and $$W_E$$ denote the group of vertexes\nodes, edges\links and edge's weight respectively. For this tutorial let's assume an undirected graph $$G$$ with $$N$$ nodes $$V$$ and $$M$$

A more involved idea is defining signals on a graph. Intuitively, a signal $$s$$ on graph $$G$$ can be seen as a vector of scalars over the nodes , and formally

$$
s:V\rightarrow \mathbb{R}^N
$$

This allows us to take both graph structure and data into for representing a phenomena.  For instance, in the context of neuroscience we could refer to a graph as the cortical network with connections/edges over functional brain regions,  while the spatio-temporal activity map, which holds the amount of neural activity for a region in time,  is interpreted as the signal $$s$$.

![enter image description here](https://lh3.googleusercontent.com/fXTZa7nClEN2A2S-CkOlEozh3nEEJY2glX9HKNrezFAaetEf4NhjZegVighwGQ6WEa0T8N00lneQ=s-1 "Illustration for a signal on a graph")

### The Shift - Signal Processing and Graph Processing
The tale of graph signal processing cannot be property introduced without discussing the **role of shift**. In Discrete Signal processing (DSP), the $$z$$-transform represents any finite signal, $$s$$ with $$N$$ samples, as a polynomial expression in the frequency domain. Formally, $$s(z)=\sum_{n=0}^{N-1}s_n z^{-n}$$ So why can we refer to it as a *shift*[^1]? Let's take for example the$$Z\text{-transform}$$ of an impulse response at $$0$$, also known as the [Dirac's delta](https://en.wikipedia.org/wiki/Dirac_delta_function)  $$\delta[n]$$ (in DSP $$\delta[n-k]$$ is a fully flat zero signal with 1 in the $$k$$-th location).

$$
Z(\delta[n])=z^0=1
$$

Now,  when multiplying the expression with $$z^{-1}$$ the new transform will be

$$
z^{-1}=Z(\delta[n-1])
$$

This means that multiplying by $$z^{-N}$$ is effectively delaying or shifting the signal with $$N$$ samples. Notice that choosing $$z^{-n}=\frac{1}{\sqrt{N}}e^{-j\frac{2\pi}{N}kn}$$ leads to the $$k$$th Fourier coefficient of signal $$s$$.
**[Finite Impulse Response (FIR)](https://en.wikipedia.org/wiki/Finite_impulse_response]) filters can be defined similarly in the $$Z$$ domain s.t. filtering becomes a product of the shifted representation of the signal $$s$$ and the filter $$h_{\text{shift}}$$.

Periodic time-series, which are a popular substance for analysis in DSP, manifest a graphical structure of a directed cyclic graph, where $$s[n]=s[n+N]$$. In these structure, points relate by their order, and specifically the timeshift, where the graph adjacency matrix has the following form

$$
    A = \begin{pmatrix}
    0 & 0& ...& 1 \\
    1 & 0& ...& 0 \\
    \vdots & \vdots& \ddots& \vdots \\
    1 & ...& 1& 0
    \end{pmatrix}
$$

This cyclic graph operator defines the unidirectional information flow in the graph, and when multiplied by a signal $$s$$ it adds the structural/time context to the set of values.
Now, we can generalize this notion to other and more complex graphs using the information of the adjacency matrix. Similarly to the DSP definition, the *shift* can bee seen as a local operator which replaces the value of every node by a linear combination of the values of its neighbors.

![
](https://lh3.googleusercontent.com/IeuaY5ef97rX6shReNlP4zjkKKSu25ldyjo4fs_q3lOFf1QXw7nQT-JwhgZHZfCMt1ZnaDfn7ru6 "illustration of graphical structure of cyclic time series")

The adjacency matrix is a
$$A\in \mathbb{R}^{|V| \times |V|}$$ symmetric matrix with entries equal to zero, $$a_{i,j}=0$$
, if there is no link between node $$i$$ and node $$j$$. There are different variations of the adjacency matrix that can be adapted as the *shift*, when popular choices are the **non-normalized combinatorial graph Laplacian**, $$L=D-A$$ and the **symmetric normalized Laplacian**, $$\mathcal{L}=D^{-1/2}LD^{-1/2}$$, where $$D$$ is the degree matrix, a diagonla matrix with

$$
D_{i,j} = \left\{ \begin{array}{rcl} \sum_{j=1}^{|V|}a_{i,j} & \text{if} & i=j\\0 & \text{else} \end{array}\right.
$$

Each shift version offers different trades-off (see [Ortega et al. 2018](https://arxiv.org/pdf/1712.00468.pdf)). Similarly to the one-dimension Laplace operator, the graph Laplacian can be seen a high-pass filter or difference between a signal on a node to the signals of it's neighborhood.

![enter image description here](https://lh3.googleusercontent.com/dhNWhyUxL4FGqiMwmUQhFglzS0rE-tigV-6rjy41PfXv0lB7DN123r7iDK1a7HoaAgfKc0wwAx2H=s0 "The graph Laplacian")

### Graph frequencies and the Graph Fourier Transform
One of the novel features in GSP is the concept of frequencies decomposition ,where the graphical structure  is decomposed into $$n$$ linearly-independent and distinct eigengraphs. One great advantage of this decomposition technique is that the estimated frequencies, or the graph Spectrum, are a totally invariant representation to node permutations. This is an highly desired property that defines an important constraint for graph learning tasks (see for example [Simonovsky et al. 2018](https://arxiv.org/pdf/1802.03480.pdf) )  .In different from traditional DSP, in GSP the decomposition is typically carried over the graph Laplacian and not over the signal, as the later does not hold any structural information. Formaly, the laplacian
$$\mathcal{L}$$ is a real symmetric and therefore can be decomposed to the quadratic form

$$\mathcal{L}=\chi\Lambda\chi^T$$
 where the columns of $$\chi$$ are the eigenvectors and $$\Lambda$$ is a diagonal matrix with $$\lambda_i$$ denoting the relative power of the $$i$$th eigenvector. Intuitively,  eigenvector $$\chi_i$$ can be though as a Fourier's basis function, $$e^{j\omega x}$$, and $$\lambda_i$$ is the frequency of this function when low frequency indicates higher smoothness/regularity.

!["visual example of graph frequencies (Ortega and Dong slides)"](https://lh3.googleusercontent.com/qtneJTQJoJ8vs7lHIa_9PL30NLWRQ6oNr3onkBLOvxjzKgaPtTvYV3HkeKMywsVBYp56XSrfRjbVsZRbtyRjuiDfHgmi8jC8nfR1mnAo7FQ45r3ofyn884rCoS1R5zzZex4RKZpb5VMppSC7vP0K6S1JmPuuMWo5Mzkjp4m5GKhZa9fFW6clJi8xe1_JTnNI4H3Q9gNbObhZesNpvOt4v9pfbWMfzQmTtsRoHcVIxCrj2915vDxZ-1MQhkVAp21ZXayNFfI9ERsComLapzEXY2Uzii45Z-I7u8kzbPm1cB-f5TKuuLVBg3mN823A--kp9XB0VMciJ1MXj78hfS9x7jLUoBisCENrtaOF0kMdnd8BXCPnqRNt3-XfERyR1c1qBmKFxMEW3eJ8BCvc9sJ9lDlkcXxrOGeMLq-usxYyXhUx9m9Z3qpXiAgAPUZ29ABC9p5ygir2VZsx5yQEooF0ZMD1EjjSGjGtI2TloPsqlw8CJ2EjjM-HccrnkyNuP21H72Tytkc5EVM6dk05qSUMSEaPdOAQaAjVMsqyROQTfPMbdimLcBtBMfZpNJ7wq_7P_blXaSSJB2h-LQU96k4PcPjpCLKIDgIBXRE8Htue-e9wzS_E0zoaC0T7vCd9rA0=w960-h540-no)
In case you are familiar with the domain of _spectral graph theory_, you might recognize that the Rayleight quotient is closely related to this decomposition. Specifically, if the eigenvectors, $$\chi_i$$, are normalized, the minimum argument $$\chi_i$$ and its corresponding value $$\lambda_i$$ describe the quotient

$$\lambda_i = \min_{\chi_i} \frac{\chi^T\mathcal{L}\chi}{\chi^T\chi} $$

The Rayleight quotient is a minimization task with various uses in several fields such as physics, mechanical elasticity, control theory, statistics and data science (see [Bai et al. 2018](http://www.unige.ch/~dlu/publications/rbstrq_preprint.pdf)). Personally, I was first introduced with the minimization problem when I studied Spectral clustering and the Graph Cut problem for image segmentation. In classical computer vision, this task was optimizing a binary vector which divided the Image Laplacian into two or more cuts, thereby separating the image to distinct segments (see [Shi & Malik, 2000](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf)).  Accordingly, spectral decomposition can be intuitively related to node clustering in graphical networks by observing low-pass frequencies.

The complex exponentials, which are the basis functions of the common Fourier transform (FT), are in fact the eigenfunctions of the one-dimensional Laplace operator, or alternatively the time-domain Laplacian (see [code example](#td_code_example)). Accordingly, the eigenvectors of graph Laplacian can be used to define the analogous Graph Fourier Transform of any signal $$s$$ over graph Laplacian $$\mathcal{L}$$
$$\mathcal{F}_{\mathcal{L}}G(s)=\chi^Ts=[\chi_1^Ts,\chi_2^Ts,...,\chi_{N-1}^Ts] \ \ \ \ \ \text{s.t.}\ \ \ \chi_i\mathcal{L}\chi_i^T=\lambda_i $$
A more straightforward comparison between the FT in DSP and the FT in graphs is demonstrated in the following equation:

$$\text{DSP -  \ \ \ }\mathcal{F}_\omega(s)=\int(e^{j\omega t})^*s(t)dt\\\text{GSP -  \ \ \ }\mathcal{F}_{\ell}(s)=\sum_i\chi_\ell^*(i)s(i)di
$$ when $$\ell$$ is analogous to the frequency $$\omega$$.

### Filtering graph signals - Spectral and Vertex domain
In this point, we have defined graph signals both in the frequency and the vertex domain, when the latter is the graph-equivalent term to the _time-domain_ in 1 dimensional signals. An interesting property of time-domain spectral decomposition is that the decomposed signal can be grasped as a linear combination of complex exponentials, where each coefficient denotes the "influence" of the frequency component. Filtering is a basic task which allows to isolate the contribution of certain frequencies, and therefore enables to remove noise or observing dynamics with in specified range of frequency. By defining the GFT, we can use the famous _Convolution Theorem_ to perform spectral filtering. In DSP, this theorem states that under suitable conditions the Fourier transform of a **convolution** of two signals is the pointwise product of their Fourier transforms. Accordingly, we can filter on input signal $$s_{in}$$ in the following manner

$$s_{out}=H\cdot s_{in}=\chi h(\Lambda)\chi^T s_{in}\\=\chi \cdot \text{diag }\Big[h(\lambda_0),...,h(\lambda_{N-1})\Big]\cdot \hat{s_{in}}$$

when $$\hat s_{in}=\chi^T s_{in}$$ is the Fourier transform of $$s_{in}$$ and $$\chi$$ is the inverse GFT. It is easier to identify the classic time-domain equations when looking on non-matrix notations

$$\text{DSP -\quad}
s_{out}(t)=\int \hat s_{in}(\omega)\hat h(\omega)e^{j\omega t}=(s_{in}*h)(t)
\\\text{GSP - \quad}
s_{out}(i)=\sum_{\ell=0}^{N-1} \hat s_{in}(\lambda_{\ell})\hat h(\lambda_{\ell})\chi_{\ell}(i) \qquad \qquad
$$

Obviously, This filtering method is considered naive, as we need to calculate the eigen-decomposition of $$\mathcal{L}$$. This step has high computational cost and is very inefficient in large networks. This problem can be relaxed by using polynomial filters, which do not require the GFT basis. With this filter we approximate the frequency response, $$h(\lambda)$$ as a linear combination of $$K$$ eigenvalues s.t.

$$\hat h(\lambda_{\ell})\approx \sum^{K-1}_{k=0}a_k\lambda_{\ell}^k$$

This approximation demand only the computation of $$K$$ frequencies, which has comes with small cost than full spectrum computation of the shift $$\mathcal{L}$$.  With little mathematical effort we can describe the filtered signal on some node $$i$$ with a polynomial of the shift

$$
s_{out}(i)=
\sum_{\ell=0}^{N-1} \hat s_{in}(\lambda_{\ell})\hat h(\lambda_{\ell})\chi_{\ell}(i) \\
=\sum_{j=1}^{N} s_{in}(j)\sum^K_{k=0}a_k\sum_{\ell=0}^{N-1} \lambda_{\ell}^k\chi^*_{\ell}(j)\chi_{\ell}(i) \\
=\sum_{j=1}^{N} s_{in}(j)\sum^{K-1}_{k=0}a_k (\mathcal{L}_{i,j})^k
$$

Effectively, we only need to calculate the $$K$$-th power of the shift in order to filter the signal! Now let's take a closer look on $$(\mathcal{L}_{i,j})^k$$.  If we decide to work with the normalized adjacency matrix (normalizing by the degree of each node), It can be shown that $$\mathcal{L}_{i,j}$$ denotes the probability of arriving in a 1-step random walk from node $$i$$ to node $$j$$ (see Lemma 5.2 in [Hammond et al. 2009](https://arxiv.org/abs/0912.3848) ).  When raised to the power of $$k$$ this term describes the following probability

$$(\mathcal{L}_{i,j})^k = \Pr(i_{\text{start}}\rightarrow j_{\text{end}}|\text{num steps}_{i\rightarrow j}=k)$$

This means that by using the following approximation we can encode the filtered signal of vertex $$i$$ with structural information of its $$K$$-hop environment. This $$K$$-localized formulation enables a nice interpretation in the vertex domain.
### Implementation via approximation
Polynomial approximation for localized graph filters are intended for  saving computations, and specifically reduce the Eigen-decomposition and multiplication. There are several popular approximations in GSP - Minmax, Meyer, Least Squares, Lanczos method and more. However, the most popular approximation is known as the [Chebysev expansion](https://en.wikipedia.org/wiki/Chebyshev_polynomials). We won't dive deeply to its rigorous definition, but only highlight that it is highly efficient computation-wise due to its recursive formulation

$$\hat h(\lambda_{\ell})\approx \sum^{K-1}_{k=0}a_kT_k(\tilde\Lambda)
\quad \text{s.t.}\quad
T_k(x)=2xT_{k-1}(x)-T_{k-2}(x)$$

You can read more about the Chebysev and other expansions in [Hammond et al. 2009](https://arxiv.org/abs/0912.3848) and [Vandergheynst & Shuman 2011](https://www.macalester.edu/~dshuman1/Talks/Vandergheynst_Shuman_Marseille_11_17_2011.pdf)


## Practice GSP with python
### First  example - Time-series Laplacian and the Fourier transform <a id='td_code_example'></a>
I will first demonstrate visually how the time-series analysis relates to GSP. Notice that plotting code is removed for brevity, but can be found in this [gist](#full_notebook).

First make sure that you downloaded `pygsp`
```bash
pip install pygsp, networkx, graphviz
```
Now, lets create a short time-series structure of eight timestamps
```python
N = 8
G = nx.Graph([(n,n+1) for n in range(N)])
```
![png](..\img\gsp_img\output_3_0.png)

The Laplacian of such a structure look like
```python
L = nx.laplacian_matrix(G)
```
![png](..\img\gsp_img\output_4_0.png)

We see that the Laplacian has actually the form of the **Discrete Cosine Transform** ([DCT](https://www.mat.univie.ac.at/~kriegl/Skripten/CG/node51.html)), which is a variant of Fourier Transform. Thus the Eigenvalues have the following analytical structure
$$\chi_k:j\rightarrow\cos\Big(\big(j+\frac{1}{2}\big)\frac{k\pi}{N}\Big)$$

```python
N=9
lambd, chi = np.linalg.eigh(L.toarray())
x = np.linspace(0, N-1,100)
dct2_eigen = lambda l: np.sqrt(2/N) * np.cos(np.pi * l * (x+0.5) / N)

# abbreviated plotting code
for k, eigvec in enumerate(chi.T):
    # scaling the first eigenvector to unit $\mathbf{1}$
    if k==0:
        axes[k].plot(x, np.ones_like(x),**kw_plot_1)
        axes[k].plot(np.ones_like(eigvec), **kw_plot_2)
        axes[k].set_title("Eigen values of time-domain laplacian vs. DCT-2 eigenvalues"
                          , fontsize=16, fontname='serif')
    else:
        axes[k].plot(x, dct2_eigen(k),**kw_plot_1)
        axes[k].plot(eigvec, **kw_plot_2)
    # TODO: bug with k=6 (!?)
    if k > 5:
        break
```
![png](..\img\gsp_img\output_6_0.png)

### Second  example - Filtering graph signal of distance measurements <a id='td_code_example'></a>
For the second example we used the famous Minnesota road map graph. This graph holds 2642 intersections and 3304 roads. It is constructed modeling the intersections as nodes and the roads as undirected edges.
We begin by visualizing the graph with geo-spatial context. Every node is located according to it's coordinate features. Remember, the graphical structure itself is solely the connections of the nodes, so that in its processing we are unaware to the geo-spatial location of the node, but only to it's connections. Here we see the graph visualized with and without the geo-spatial context
```python
from pygsp import graphs, filters
from pygsp import plotting
import matplotlib.pylab as plt

G = graphs.Minnesota()
G_nx = nx.Graph([tuple(r) for r in np.array(G.get_edge_list()[:2]).T])
G.compute_fourier_basis() # caching fourier basis
```
![png](..\img\gsp_img\output_8_1.png)

Next, lets create a noisy signal based on the distance from the dense part of Minnesota, specifically in $$(-93.2, 45)$$, somewhere next to Hennepin. We added a nonlinear cutoff for $s_{in}>2$ ,to further localize the signal.

```python
rs = np.random.RandomState()
s = np.zeros(G.N)
s += np.sqrt(np.sum((G.coords - np.array([-93.2, 45]))**2, axis=1))
s[s>2] = 3
s += rs.uniform(-1,1, size=G.N)
```
![png](..\img\gsp_img\output_10_0.png)

First let's design a heat filter and visualize it in the spectral domain.
The impulse response of this filter is

$$\hat{g}(x) = \exp \left( \frac{-\tau x}{\lambda_{\text{max}}} \right)$$

As you can tell, this is a Low Pass filter. We choose $$\tau=50$$ because we want to assure the removal of higher noisy frequencies, and we assume that the spectral components that describe the clean signal have very small eigenvalues.
```python
# First design LPF filter as a heat kernel
g = filters.Heat(G, tau=50)
```

![png](..\img\gsp_img\output_12_0.png)

Next, we visualize the effect of the filter on the noisy signal

```python
s_out = g.filter(s, method='exact') # exact uses GFT. Chebychev approx. is also available

# plotting code
fig, axes = plt.subplots(1, 2, figsize=(13, 6))
axes[0]`
plotting.plot_signal(G, s, ax=axes[0], **plottingparam)
_ = axes[0].set_title('Noisy signal before filtering',fontsize=15)
axes[0].set_axis_off()
plotting.plot_signal(G, s_out, ax=axes[1], **plottingparam)
_ = axes[1].set_title('Filtered signal',fontsize=15)
axes[1].set_axis_off()
fig.tight_layout()
plt.show()
```
![png](..\img\gsp_img\output_13_0.png)


### Applications

 [^1]: A very nice and intuitive explanation on the $Z\text{-transform}$ can be found [here](https://www.quora.com/Intuitively-speaking-what-does-a-z-transform-represent).

### Resources
+ [*Graph Signal Processing: Overview, Challenges and Applications*](https://arxiv.org/pdf/1712.00468.pdf)- Ortega et al. 2018
+ [*On the Shift Operator, Graph Frequency, and Optimal Filtering in Graph Signal Processing*](https://www.google.co.il/search?q=On+the+Shift+Operator%2C+Graph+Frequency%2C+and+Optimal+Filtering+in+Graph+Signal+Processing&oq=On+the+Shift+Operator%2C+Graph+Frequency%2C+and+Optimal+Filtering+in+Graph+Signal+Processing&aqs=chrome..69i57j69i61j0j69i64.621j0j7&sourceid=chrome&ie=UTF-8) - Gavili & Zhang 2017
+ [Graph Signal Processing: Filterbanks, Sampling and Applications to Machine Learning and Video Coding](http://158.132.21.62/apsipa2015/Keynote%20Speeches/GSP-Intro-Learning-Dec-2015.pdf) Ortega 2015 [youtube link](https://www.youtube.com/watch?v=AKxwZaEavRw&list=PLdduXA5_uiHgiET0OJwggUacesLC5mCyt)
+ [Normalized Cuts and Image Segmentation](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf) - Shi and Malik ,2000
+ [GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders](https://arxiv.org/pdf/1802.03480.pdf) - Simonovsky et al 2018
+ [Graph signal processing workshop](https://gsp17.ece.cmu.edu/program/)

{% gist fd5ec26b238802d79cfd54f632c55897%} <a id='full_notebook'></a>