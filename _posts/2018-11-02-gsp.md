---
layout: post
title: A Signal Processing Perspective For Graph Structured Information
image: ..\img\gsp_img\post_img.png
share-img: ..\img\gsp_img\post_img.png
show-avatar: true
<!-- bigimg: ..\img\gsp_img\graph_signal_example.png -->
subtitle: Introduction to Graph Signal Processing
use-site-title: true
tags:
  - signal-processing
  - graph
  - spectral domain
published: true
---

<!-- bigimg:    # /path/to/img  -->
> TL;DR: In this post we will learn the basic concepts of linear graph signal processing and see how they're related to classical signal processing. By the end of this post the reader will understand what are graph signals, their relation to time domain signals, representation of graph signal in the vertex and the spectral domain, Graph Fourier transform and efficient spectral filtering. 

Even though relevant material is mostly linked, basic background in Fourier analysis, Spectral analysis and Convolution theory may simplify the read.
{: .box-warning}

Data appears in various shapes. On the one side we have natural scenes, such as in images, motion trajectories, video or sound, which are unstructured and typically demonstrate continuities, smoothness and regularity. These high-level constraints impose prior knowledge that can be utilized and formulated for modeling these signals. On the other side we meet structured data sources, such as tables and graphs, with discrete associations, irregular properties and relational structure. These structures may seem more complicated and harder to grasp, however their unique organization is highly compact, very efficient and great for search. Typically,a graph's connectome ,or wiring network, holds rich information on its entities, and thus becomes an interesting subject in many different contexts. 
Formally, relational graph data is represented with $$G=\{V,E,W_E\}$$ where $$V$$, $$E$$  and $$W_E$$ denote the group of vertexes (or nodes), edges (links or connections) and edge's weight respectively. In order to make life a bit easier, the graphs in this discussion are undirected $$G$$ with $$N$$ nodes and $$E$$ edges.

When overviewing the field of GSP, the first question to be asked would probably be - How do we define a signal over a graph?
Intuitively, a signal $$s$$ on graph $$G$$ can be seen as a vector of scalars over the nodes , and formally

$$
s:V\rightarrow \mathbb{R}^N
$$

Graph-structured data enables to recruit both the network structure and the features over nodes for solving difficult tasks.  For instance, in Neuroscience the Cortical network acts as the graph, with edges connecting functional brain regions, when each region is characterized by its sensory association, location, area and response pattern to various stimuli. In this post I want to show you that various tools and concepts from traditional signal processing can be applied to graph data, if only we'll associate the graph's architacture with the time-axis. 

<p align="center">
<img src="..\img\gsp_img\graph_signal_example.png" width="500px" height="275px">
 <figcaption><p align="center">
Simple graph signal example</p> </figcaption>
</p>

### Background - The shift operator in Signal Processing
We will start by explaining the shift and its role in DSP and GSP. From now on we will use $$s$$ for a signal with $$N$$ samples. In Discrete Signal processing (DSP), we can represent any causal and finite signal with the [$$z$$-transform](https://en.wikipedia.org/wiki/Z-transform) as a polynomial expression, s.t. 

add intuition of moving from time domian to spectral domain in the 
{: .box-error}


$$Z(s[n])=\sum_{n=0}^{N-1}s_n z^{-n}$$ 

This representation is usually refereed as the signal's _shift_[^1], when basically, <u>it is one of the fundamental building blocks of modern signal processing</u>, which allows representing signals and designing filters by finite degree polynomials. The transform is essentially expanding a signal to a scaled power series, or a [Laurent series](https://en.wikipedia.org/wiki/Laurent_series). In order to gain some intuition for the naming, we'll explore the $$z$$-transform of the discrete [Dirac's delta function](https://en.wikipedia.org/wiki/Dirac_delta_function) $$\delta[n-k]$$, which is a fully flat signal of zero's with 1 in the $$k$$-th location. This means that the $$z$$-transform of $$\delta[n-0]$$ is 

$$Z(\delta[n])=1\cdot z^0+0\cdot z^{-1} +0\cdot z^{-2} +...=z^0=1$$

Now, let's see what happens when this expression is multiplied by $$z^{-1}$$. The new transform will be 
$$z^{-1}=Z(\delta[n-1])$$.
It appears that this manipulation is effectively time-delaying, or _shifting_, the signal by one sample. In the same manner, using $$z^{-N}$$ will delay in $$N$$ samples. Notice that choosing $$z^{-n}=\frac{1}{\sqrt{N}}e^{-j\frac{2\pi}{N}kn}$$ leads to the frequency representation of signal $$s$$, and particularly the Discrete Fourier Transform (DFT).
In DSP the shift is the foundation for building many linear filters, such as the [Finite Impulse Response (FIR)](https://en.wikipedia.org/wiki/Finite_impulse_response) filter family, which are defined similarly as $$h(z)=\sum_{n=0}^{N-1}h_n z^{-n}$$. 

make the Z-transform concept more intuitive. Differentiate between filters and delta. Should the $$h(z)$$ be mentioned?
{: .box-error}


### The Shift operator in Graphs
In order to define the GSP counterpart we will start by defining the shift operation on a graph signal of a periodic time-series $$s=[s_0,s_1,...,s_{N-1}]^T$$. This signal manifest a graphical structure of a directed cyclic graph, where $$s[n]=s[n+N]$$. 

<p align="center">
<img src="..\img\gsp_img\time_domain_graph.png" width="500px" height="200px">
 <figcaption><p align="center">
 Graph  structure of a periodic time-series </p> </figcaption>
</p>

The signal can be periodically shifted by multiplying it with 
$$A\in \mathbb{R}^{|V| \times |V|}$$
, a cyclic shift matrix s.t. 
$$s_{\text{shifted}}=A\cdot s$$ when

$$
    A = \begin{pmatrix}
    0 & 0& ...& 1 \\
    1 & 0& ...& 0 \\
    \vdots & \vdots& \ddots& \vdots \\
    0 & ...& 1& 0
    \end{pmatrix}
$$

This cyclic graph operator is precisely the _adjacency matrix_ of the above periodic time-series graph. The adjacency matrix is a symmetric matrix with entries equal to zero, $$a_{i,j}=0$$, if there is no link between node $$i$$ and node $$j$$. In our context, it defines the unidirectional information flow in the graph and supplies time-context to the set of graph-signal values.
Accordingly, the *shift* in GSP can be seen as a local operator which replaces the value of every node by a linear combination of the values of its neighbors.
Other choices and variations for the graph shift operation have been proposed, when popular choices are the _non-normalized combinatorial graph Laplacian_, 
$$L=D-A$$
and the _symmetric normalized Laplacian_, 
$$\mathcal{L}=D^{-1/2}LD^{-1/2}$$
, where $$D$$ is the degree matrix, a diagonal matrix with

$$
D_{i,j} = \left\{ \begin{array}
{rcl} \sum_{j=1}^{|V|}a_{i,j} & \text{if} & i=j\\0 & \text{else} 
\end{array}\right.
$$

Each shift version offers different trades-off (see [Ortega et al. 2018](https://arxiv.org/pdf/1712.00468.pdf)). Similarly to the one-dimension Laplace operator, the graph Laplacian can be seen a high-pass filter or difference between a signal on a node to the signals of it's neighborhood.

Explain a bit more why multipling by A is a shift in the graph world and what does it represent and effect? Multipling by L - what is he's effect? why D-A? what is the main difference between them?.
{: .box-error}

<figure>
    <p align="center">
    <img src="..\img\gsp_img\laplacian_viz.png" alt="The graph Laplacian">
     <figcaption><p align="center">
     The graph Laplacian </p> </figcaption>
    </p>
</figure>


{% include conclusion.html content=
    "<br/>+ The $$z$$-transform is a key representation for time and frequency analysis in classical DSP.
    <br/> + It relies on the definition of the _shift_, or time-delay, operation.
    <br/> + The analogy for the shift operation in GSP is a product of the graph signal with a matrix that holds its structural information, such as the adjacency matrix or the Laplacian.
" %} 

### Graph frequencies and the Graph Fourier Transform

Now that we understand the role of shift in spectral representation and its definition in GSP, it is time to discuss the notion of graph frequencies. You may be already familiar with frequencies in time-domain, which are typically described as sinusoid oscillating at low to high rates. In GSP, low and high frequencies describe the amount of change between values of connected nodes in the graphs frequency components. The change itself is measured using _graph total variation_, which quantifies the total difference between a graph signal and its shifted version (see [Sandryhaila & Moura 2014](https://arxiv.org/pdf/1307.0468.pdf)).

The graph frequency, or spectral components, are obtained via Spectral decomposition ,where the graphical structure  is decomposed into $$n$$ linearly-independent and distinct eigengraphs. One great advantage of this technique is that the estimated frequencies, or the graph spectrum, are totally invariant to node permutations. This is an highly desired property that defines an important constraint for graph learning tasks (see for example [Simonovsky et al. 2018](https://arxiv.org/pdf/1802.03480.pdf)) .In different from DSP, here the transform is not universal, as the connection structure in the shift varies between graphs, and specific decomposition is needed for each graph format. For instance, the Laplacian shift
$$\mathcal{L}$$
is a real symmetric matrix and therefore can be decomposed to the quadratic form

What is are eigen-graphs?
{: .box-error}

$$\mathcal{L}=\chi\Lambda\chi^T$$

where the columns of $$\chi$$ are the eigenvectors and $$\Lambda$$ is a diagonal matrix with $$\lambda_i$$ denoting the relative power of the $$i$$-th eigenvector. Intuitively,  eigenvector $$\chi_i$$ can be though as a Fourier's basis function, $$e^{j\omega x}$$, and $$\lambda_i$$ is the frequency of this function ,when the lower the frequency, the more regular or smooth the signal is.

why? what is the mutual connection between the Fourier basis function and the eigenvectors. Also, mention the zero crossing here, for frequency intuition
{: .box-error}

<figure>
    <p align="center">
    <img src="..\img\gsp_img\decomp_graph.svg">
     <figcaption><p align="center">
     Visual example of graph frequencies (Ortega and Dong slides) </p> </figcaption>
    </p>
</figure>

In case you are familiar with the domain of _spectral graph theory_, you might recognize that the Rayleight quotient is closely related to this decomposition. Specifically, if the eigenvectors, $$\chi_i$$, are normalized, the minimum argument $$\chi_i$$ and its corresponding value $$\lambda_i$$ describe the quotient

$$\lambda_i = \min_{\chi_i} \frac{\chi^T\mathcal{L}\chi}{\chi^T\chi} $$

The Rayleight quotient is a minimization task with various uses in several fields such as physics, mechanical elasticity, control theory, statistics and data science (see [Bai et al. 2018](http://www.unige.ch/~dlu/publications/rbstrq_preprint.pdf)). Personally, I was first introduced with the minimization problem when I studied Spectral clustering and the Graph Cut problem for image segmentation. In classical computer vision, this task was optimizing a binary vector which divided the Image Laplacian into two or more cuts, thereby separating the image to distinct segments (see [Shi & Malik, 2000](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf)). The first non trivial solution to this partition is the binarized [_Fiedler vector_](https://en.wikipedia.org/wiki/Algebraic_connectivity) - the second eigenvalue of the quadratic form. Accordingly, spectral decomposition can be intuitively related to node clustering in graphical networks by observing low-pass frequencies.

<u>The complex exponentials</u>, which are the basis functions of the common Fourier transform (FT), <u>are in fact the eigenfunctions of the one-dimensional Laplace operator</u>, or alternatively the time-domain Laplacian (see [code example](#td_code_example)). Accordingly, the eigenvectors of graph Laplacian can be used to define the analogous Graph Fourier Transform of any signal $$s$$ over graph Laplacian $$\mathcal{L}$$

$$
\mathcal{F}_{\mathcal{L}}(s)=\chi^Ts=[\chi_1^Ts,\chi_2^Ts,...,\chi_{N-1}^Ts] \ \ \ \ \ \text{s.t.}\ \ \ \chi_i\mathcal{L}\chi_i^T=\lambda_i 
$$

An explicit comparison of the Fourier Transform in time and graph domain, facilitates the analogy even more:

$$\text{DSP - }\ \ \ \mathcal{F}_\omega(s)=\int(e^{j\omega t})^*s(t)dt\\\text{GSP - }\ \ \ \mathcal{F}_{\ell}(s)=\sum_i\chi_\ell^*(i)s(i)di
$$

when $$\ell$$ is analogous to the frequency term $$\omega$$. Thus, the Fourier transform of $$s$$ can be abbreviated to $$mathcal{F}(s)=\chi^T s$$


{% include conclusion.html content=
    "<br/>+ Eigenvalue decomposition of the shift matrix leads to the spectral representation of graph signal.
    <br/> + High and low frequencies of graph signals translate to the number of zero-crossings in the graph spectral components.
    <br/> The Fourier basis is the eigenbasis of the time-domain graph's Laplacian.
" %} 

### Filtering graph signals - Spectral and Vertex domain

In this point, we have defined graph signals both in the frequency and the vertex domain. An interesting property of time-domain spectral decomposition is that the decomposed signal can be grasped as a linear combination of complex exponentials, where each coefficient denotes the "influence" of the frequency component. Filtering is a basic task which allows to isolate the contribution of certain frequencies, and therefore enables to remove noise or observe dynamics in a specified spectrum. By defining the Graph Fourier Transform (GFT), we can use the famous _Convolution Theorem_ to perform spectral filtering. In DSP, this theorem states that under suitable conditions "_the Fourier transform of a convolution of two signals is the pointwise product of their Fourier transforms_". Accordingly, we can filter on input signal $$s_{in}$$ in the following manner

$$s_{out}=H\cdot s_{in}=\chi h(\Lambda)\chi^T s_{in}\\=\chi \cdot \text{diag }\Big[h(\lambda_0),...,h(\lambda_{N-1})\Big]\cdot \hat{s_{in}}$$

when $$\hat s_{in}=\chi^T s_{in}$$ is the Fourier transform of $$s_{in}$$ and $$\chi$$ is the inverse GFT. It is easier to identify the classic time-domain equations when looking on non-matrix notations

$$
\text{DSP - }\ \ \ \ 
s_{out}(t)=\int \hat s_{in}(\omega)\hat h(\omega)e^{j\omega t}=(s_{in}*h)(t)
\\\text{GSP - }\ \ \ \ 
s_{out}(i)=\sum_{\ell=0}^{N-1} \hat s_{in}(\lambda_{\ell})\hat h(\lambda_{\ell})\chi_{\ell}(i) \qquad \qquad
$$

Obviously, This filtering method is considered naive, as we need to calculate the eigen-decomposition of $$\mathcal{L}$$. This step has high computational cost and is very inefficient in large networks. This problem can be relaxed by using polynomial filters, which do not require the GFT basis. With this filter we approximate the frequency response, $$h(\lambda)$$ as a linear combination of $$K$$ eigenvalues s.t.

$$\hat h(\lambda_{\ell})\approx \sum^{K-1}_{k=0}a_k\lambda_{\ell}^k$$

This approximation requires only the computation of $$K$$ frequencies, which is less heavy than the computation of the full spectrum of the shift $$\mathcal{L}$$.  With little mathematical effort one can describe the filtered signal on node $$i$$ with a power series of the shift

$$
s_{out}(i)=
\sum_{\ell=0}^{N-1} \hat s_{in}(\lambda_{\ell})\hat h(\lambda_{\ell})\chi_{\ell}(i) \\
=\sum_{j=1}^{N} s_{in}(j)\sum^K_{k=0}a_k\sum_{\ell=0}^{N-1} \lambda_{\ell}^k\chi^*_{\ell}(j)\chi_{\ell}(i) \\
=\sum_{j=1}^{N} s_{in}(j)\sum^{K-1}_{k=0}a_k (\mathcal{L}_{i,j})^k
$$


(where did $$\chi^*_{\ell}$$ in the second equation come from?) + explain the last $$=$$
{: .box-error}

Effectively, we only need to calculate the $$K$$-th power of the shift in order to filter the signal! 
Now let's take a closer look on 
$$(\mathcal{L}_{i,j})^k$$.  
If we decide to work with the normalized adjacency matrix (normalizing by the degree of each node), It can be shown that $$\mathcal{L}_{i,j}$$ denotes the probability of arriving in a 1-step random walk from node $$i$$ to node $$j$$ (see Lemma 5.2 in [Hammond et al. 2009](https://arxiv.org/abs/0912.3848) ).  When raised to the power of $$k$$ this term describes the following probability

$$(\mathcal{L}_{i,j})^k = \Pr(i_{\text{start}}\rightarrow j_{\text{end}}|\text{num steps}_{i\rightarrow j}=k)$$

This means that by using the following approximation we can encode the filtered signal of vertex $$i$$ with structural information of its $$K$$-hop environment. This $$K$$-localized formulation enables a nice interpretation of filtering in the vertex domain.

(this is right for L? I thought its right for A! whats the diffrenece?)
{: .box-error}

### Implementation via approximation
Polynomial approximation for localized graph filters are intended for saving computations, and specifically reduce the eigen-decomposition and multiplication. There are several popular approximations in GSP - Minmax, Meyer, Least Squares, Lanczos method and more, while the most popular approximation is known as the [Chebysev expansion](https://en.wikipedia.org/wiki/Chebyshev_polynomials). We won't dive deeply to its rigorous definition, but only highlight that it is highly efficient due to the recursive formulation of the filter

$$\hat h(\lambda_{\ell})\approx \sum^{K-1}_{k=0}a_kT_k(\tilde\Lambda)
\quad \text{s.t.}\quad
T_k(x)=2xT_{k-1}(x)-T_{k-2}(x)$$

You can read more about the Chebysev and other expansions in [Hammond et al. ,2009](https://arxiv.org/abs/0912.3848) and [Vandergheynst & Shuman, 2011](https://www.macalester.edu/~dshuman1/Talks/Vandergheynst_Shuman_Marseille_11_17_2011.pdf)

(maybe you should dive here a bit deeper? this can be the main point of this blog!)
{: .box-error}

{% include conclusion.html content=
    "<br/>+ The convolution theorem in DSP translates to practically matrix product of the signal with the spectral components of the Laplacian.
    <br/>+ Efficient implementation of graph filtering uses a power series of the Laplacian instead of eigen-decomposition.
" %} 


# Coding time! practice GSP with python and pygsp
### First  example - The time-domain Laplacian and the Fourier transform <a id='td_code_example'></a>
I will first demonstrate visually how the time-series analysis relates to GSP. <u>Notice that plotting code is removed for brevity, but can be found in this</u> [gist](#full_notebook) 

First make sure that you downloaded `pygsp`
```bash
! pip install pygsp, networkx, graphviz
```
Now, lets create a short time-series structure of eight timestamps
```python
N = 8
G = nx.Graph([(n,n+1) for n in range(N)])
```
![png](..\img\gsp_img\output_3_0.png)

The Laplacian of such a structure look like
```python
L = nx.laplacian_matrix(G)
```
<p>
    <img src="..\img\gsp_img\output_4_0.png">
</p>

We see that the Laplacian has actually the form of the **Discrete Cosine Transform** ([DCT](https://www.mat.univie.ac.at/~kriegl/Skripten/CG/node51.html)), which is a variant of Fourier Transform. Thus the Eigenvalues have the following analytical structure
$$\chi_k:j\rightarrow\cos\Big(\big(j+\frac{1}{2}\big)\frac{k\pi}{N}\Big)$$

```python
N=9
lambd, chi = np.linalg.eigh(L.toarray())
x = np.linspace(0, N-1,100)
dct2_eigen = lambda l: np.sqrt(2/N) * np.cos(np.pi * l * (x+0.5) / N)

# abbreviated plotting code
for k, eigvec in enumerate(chi.T):
    # scaling the first eigenvector to unit $\mathbf{1}$
    if k==0:
        axes[k].plot(x, np.ones_like(x),**kw_plot_1)
        axes[k].plot(np.ones_like(eigvec), **kw_plot_2)
        axes[k].set_title("Eigen values of time-domain laplacian vs. DCT-2 eigenvalues"
                          , fontsize=16, fontname='serif')
    else:
        axes[k].plot(x, dct2_eigen(k),**kw_plot_1)
        axes[k].plot(eigvec, **kw_plot_2)
    # TODO: bug with k=6 (!?)
    if k > 5:
        break
```
<p align="center">
    <img src="..\img\gsp_img\output_6_0.png">
</p>

This exemplifies the previous statement - the one dimensional Laplace operator, or the Laplacian of the time-domain graph, has the spectral representation of the Fourier Transform.

### Second  example - Filtering graph signal of distance measurements <a id='td_code_example'></a>

For the second example we used the famous Minnesota road map graph. This graph holds 2642 intersections and 3304 roads. It is constructed by modeling the intersections as nodes and the roads as undirected edges.
We begin by visualizing the graph with geo-spatial context. Every node is located according to it's coordinate features. Remember, <u>the graphical structure itself is solely the connections of the nodes, so that in its processing we are unaware to the geo-spatial location of the node, but only to it's connections</u>. This enables us to do very simple and <u>visible</u> geo-spatial manipulation according to the node's location. Here we see the graph visualized with and without the geo-spatial context
```python
from pygsp import graphs, filters
from pygsp import plotting
import matplotlib.pylab as plt

G = graphs.Minnesota()
G_nx = nx.Graph([tuple(r) for r in np.array(G.get_edge_list()[:2]).T]) # creating networkx graph  without geo-spatial info. This is plotted with spring layout
```
![png](..\img\gsp_img\output_8_1.png)

Next, we create a noisy signal based on the distance from the dense part of Minnesota, coordinates $$(-93.2, 45)$$, somewhere next to Hennepin county. We added a nonlinear cutoff for $s_{in}>2$ ,to further localize the signal.

```python
rs = np.random.RandomState()
s = np.zeros(G.N)
s += np.sqrt(np.sum((G.coords - np.array([-93.2, 45]))**2, axis=1))
s[s>2] = 3
s += rs.uniform(-1,1, size=G.N)
```
![png](..\img\gsp_img\output_10_0.png)

First let's design a heat filter and visualize it in the spectral domain.
The impulse response of this filter is

$$\hat{g}(x) = \exp \left( \frac{-\tau x}{\lambda_{\text{max}}} \right)$$

As you can tell, this is a Low Pass filter. We choose $$\tau=50$$ because we want to assure the removal of higher noisy frequencies, and we assume that the spectral components that describe the clean signal have very small eigenvalues.
```python
# First design LPF filter as a heat kernel
g = filters.Heat(G, tau=50)
```

![png](..\img\gsp_img\output_12_0.png)

Finally we visualize the effect of the filter on the noisy signal

```python
s_out = g.filter(s, method='exact') # exact uses GFT. Chebychev approx. is also available

# plotting code
fig, axes = plt.subplots(1, 2, figsize=(13, 6))
axes[0]`
plotting.plot_signal(G, s, ax=axes[0], **plottingparam)
_ = axes[0].set_title('Noisy signal before filtering',fontsize=15)
axes[0].set_axis_off()
plotting.plot_signal(G, s_out, ax=axes[1], **plottingparam)
_ = axes[1].set_title('Filtered signal',fontsize=15)
axes[1].set_axis_off()
fig.tight_layout()
plt.show()
```
![png](..\img\gsp_img\output_13_0.png)

Seems like the filtering succeeded. We clearly see that the filtered signal appears to be smoother and is closer to the original clean distance-based signal.

## Final words

That's all for now, hope you enjoyed taking a zip from the beautiful field of GSP. I hope you now understand the relation between spectral representation of time-domain and graph signals, and that you gained some intuition about graph frequencies in both spectral and vertex domain. Remember, this blog post is just a very basic introduction to the field. With the resources bellow, you may further dive in for wavelet filter banks, autoregressive modeling of time-graph signals or GSP applications in Big Data constraints, with the resources bellow.
Special thanks to [Uriel Singer]() and [Benny Peretz]() for reviewing and refining this post.

 [^1]: A very nice and intuitive explanation on the $$z$$-transform can be found [here](https://www.quora.com/Intuitively-speaking-what-does-a-z-transform-represent).

### Resources
[*Graph Signal Processing: Overview, Challenges and Applications*](https://arxiv.org/pdf/1712.00468.pdf)- Ortega et al. 2018

[*On the Shift Operator, Graph Frequency, and Optimal Filtering in Graph Signal Processing*](https://www.google.co.il/search?q=On+the+Shift+Operator%2C+Graph+Frequency%2C+and+Optimal+Filtering+in+Graph+Signal+Processing&oq=On+the+Shift+Operator%2C+Graph+Frequency%2C+and+Optimal+Filtering+in+Graph+Signal+Processing&aqs=chrome..69i57j69i61j0j69i64.621j0j7&sourceid=chrome&ie=UTF-8) - Gavili & Zhang 2017

[Big Data Analysis with Signal Processing on Graphs](https://users.ece.cmu.edu/~asandryh/papers/spm14.pdf) - Sandryhaila & Moura 2014

[Discrete Signal Processing on Graphs:
Frequency Analysis](https://arxiv.org/pdf/1307.0468.pdf) - Sandryhaila & Moura 2014

[Graph signal processing Concepts, tools and applications](http://web.media.mit.edu/~xdong/presentation/CDT_GuestLecture_GSP.pdf) - Xiaowen Dong

[Graph Signal Processing: Filterbanks, Sampling and Applications to Machine Learning and Video Coding](http://158.132.21.62/apsipa2015/Keynote%20Speeches/GSP-Intro-Learning-Dec-2015.pdf) - Ortega 2015 <!-- [youtube link](https://www.youtube.com/watch?v=AKxwZaEavRw&list=PLdduXA5_uiHgiET0OJwggUacesLC5mCyt) -->

[Normalized Cuts and Image Segmentation](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf) - Shi and Malik ,2000

[GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders](https://arxiv.org/pdf/1802.03480.pdf) - Simonovsky et al 2018

[Graph signal processing workshop](https://gsp17.ece.cmu.edu/program/)



### Original notebook gist
{% gist fd5ec26b238802d79cfd54f632c55897%} <a id='full_notebook'></a>
